{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stylegan(object):\n",
    "    def __init__(self, session, batch_size=64, output_resolution=256):\n",
    "        self.sess = session\n",
    "        self.output_resolution = output_resolution\n",
    "        self.num_style_blocks = 0\n",
    "        self.num_discriminator_blocks = 0\n",
    "        self.num_to_rgbs = 0\n",
    "        self.num_from_rgbs = 0\n",
    "        self.num_downsamples = 0\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.conv_weights = []\n",
    "        self.w_transforms = []\n",
    "        \n",
    "        self.latent_z = tf.random.normal(\n",
    "            shape=[self.batch_size,512],\n",
    "            stddev=1.0\n",
    "        )\n",
    "        \n",
    "        # This should come from a feed-forward NN.\n",
    "        #self.latent_w = tf.placeholder(\n",
    "        #    shape=[64, 512],\n",
    "        #    dtype=tf.float32\n",
    "        #)\n",
    "        \n",
    "        self.latent_w = self.latentZMapper(self.latent_z)\n",
    "        \n",
    "        self.true_images_ph = tf.placeholder(\n",
    "            shape=[None, 256, 256, 3],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.generator_out = self.evalGenerator(self.latent_w)\n",
    "        self.disc_of_gen_out = self.evalDiscriminator(self.generator_out)\n",
    "        self.disc_of_truth_out = self.evalDiscriminator(self.true_images_ph)\n",
    "        \n",
    "        self.discriminator_gradient = None\n",
    "        \n",
    "        self.loss = None\n",
    "        \n",
    "    def latentZMapper(self, Z_in, depth=8):\n",
    "        result = None\n",
    "        for i in range(depth):\n",
    "            W = tf.get_variable(\n",
    "                \"W_mapper_\" + str(i),\n",
    "                [512, 512],\n",
    "                initializer=tf.initializers.random_normal(stddev=0.3)\n",
    "            )\n",
    "            b = tf.get_variable(\n",
    "                \"b_mapper_\" + str(i),\n",
    "                [512,],\n",
    "                initializer=tf.initializers.random_normal(stddev=0.3)\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                result = tf.nn.relu(tf.matmul(Z_in, W) + b)\n",
    "            else:\n",
    "                result = tf.nn.relu(tf.matmul(result, W) + b)\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    def evalGenerator(self, W_in):\n",
    "        with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE) as vs:\n",
    "            self.constant_input = tf.get_variable(\n",
    "                \"c_1\",\n",
    "                [4, 4, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            batch_size = tf.shape(W_in)[0]\n",
    "            \n",
    "            tiled_constant_input = tf.tile(\n",
    "                tf.expand_dims(\n",
    "                    self.constant_input, axis=0\n",
    "                ),\n",
    "                [batch_size, 1, 1, 1]\n",
    "            )\n",
    "            \n",
    "            print(\"tiled constant input:\", tiled_constant_input)\n",
    "            \n",
    "            block_4_2 = self.styleBlock(\n",
    "                tiled_constant_input,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #1\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=4\n",
    "            )\n",
    "            \n",
    "            to_rgb_1 = self.toRgb(block_4_2, 4, 4, 512)\n",
    "            \n",
    "            block_8_1 = self.styleBlock(\n",
    "                block_4_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #2\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=8,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_8_2 = self.styleBlock(\n",
    "                block_8_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #3\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=8\n",
    "            )\n",
    "            \n",
    "            to_rgb_2 = self.toRgb(block_8_2, 8, 8, 512) + self.upsample(to_rgb_1)\n",
    "            \n",
    "            block_16_1 = self.styleBlock(\n",
    "                block_8_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #4\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=16,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_16_2 = self.styleBlock(\n",
    "                block_16_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #5\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=16,\n",
    "            )\n",
    "            \n",
    "            to_rgb_3 = self.toRgb(block_16_2, 16, 16, 512) + self.upsample(to_rgb_2)\n",
    "            \n",
    "            block_32_1 = self.styleBlock(\n",
    "                block_16_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #6\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=32,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_32_2 = self.styleBlock(\n",
    "                block_32_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #7\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=32,\n",
    "            )\n",
    "            \n",
    "            to_rgb_4 = self.toRgb(block_32_2, 32, 32, 512) + self.upsample(to_rgb_3)\n",
    "            \n",
    "            block_64_1 = self.styleBlock(\n",
    "                block_32_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #8\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=64,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_64_2 = self.styleBlock(\n",
    "                block_64_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=256,\n",
    "                fm_dimension=64,\n",
    "            )\n",
    "            print(\"block_64_2:\", block_64_2)\n",
    "            to_rgb_5 = self.toRgb(block_64_2, 64, 64, 256) + self.upsample(to_rgb_4)\n",
    "            \n",
    "            block_128_1 = self.styleBlock(\n",
    "                block_64_2,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=256,\n",
    "                fm_dimension=128,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_128_2 = self.styleBlock(\n",
    "                block_128_1,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=128,\n",
    "                fm_dimension=128,\n",
    "            )\n",
    "            \n",
    "            to_rgb_6 = self.toRgb(block_128_2, 128, 128, 128) + self.upsample(to_rgb_5)\n",
    "            \n",
    "            block_256_1 = self.styleBlock(\n",
    "                block_128_2,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=128,\n",
    "                fm_dimension=256,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_256_2 = self.styleBlock(\n",
    "                block_256_1,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=64,\n",
    "                fm_dimension=256\n",
    "            )\n",
    "            \n",
    "            to_rgb_7 = self.toRgb(block_256_2, 256, 256, 64) + self.upsample(to_rgb_6)\n",
    "            \n",
    "            return to_rgb_7\n",
    "            \n",
    "    def evalDiscriminator(self, rgb_in):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE) as vs:\n",
    "            from_rgb_1 = self.fromRgb(rgb_in, 256, 256, 16)\n",
    "            \n",
    "            print(\"from rgb:\", from_rgb_1)\n",
    "            \n",
    "            downsample_1 = self.downsample(from_rgb_1, 16)\n",
    "            block_128_1 = self.discriminatorBlock(\n",
    "                from_rgb_1,\n",
    "                16,\n",
    "                32\n",
    "            )\n",
    "            \n",
    "            res_1 = downsample_1 + block_128_1\n",
    "            print(\"res 1:\", res_1)\n",
    "            downsample_2 = self.downsample(res_1, 32)\n",
    "            block_64_2 = self.discriminatorBlock(\n",
    "                res_1,\n",
    "                32,\n",
    "                64\n",
    "            )\n",
    "            \n",
    "            res_2 = downsample_2 + block_64_2\n",
    "            \n",
    "            print(\"res 2:\", res_2)\n",
    "            \n",
    "            downsample_3 = self.downsample(res_2, 64)\n",
    "            block_32_3 = self.discriminatorBlock(\n",
    "                res_2,\n",
    "                64,\n",
    "                128\n",
    "            )\n",
    "            \n",
    "            res_3 = downsample_3 + block_32_3\n",
    "            \n",
    "            downsample_4 = self.downsample(res_3, 128)\n",
    "            block_16_4 = self.discriminatorBlock(\n",
    "                res_3,\n",
    "                128,\n",
    "                256\n",
    "            )\n",
    "            \n",
    "            res_4 = downsample_4 + block_16_4\n",
    "            \n",
    "            downsample_5 = self.downsample(res_4, 256)\n",
    "            block_8_5 = self.discriminatorBlock(\n",
    "                res_4,\n",
    "                256,\n",
    "                512\n",
    "            )\n",
    "            \n",
    "            res_5 = downsample_5 + block_8_5\n",
    "            \n",
    "            downsample_6 = self.downsample(res_5, 512, 512)\n",
    "            block_4_6 = self.discriminatorBlock(\n",
    "                res_5,\n",
    "                512,\n",
    "                512\n",
    "            )\n",
    "            \n",
    "            res_6 = downsample_6 + block_4_6\n",
    "            \n",
    "            #downsample_7 = self.downsample(res_6, 512, 512)\n",
    "            #block_4_7 = self.discriminatorBlock(\n",
    "            #    res_6,\n",
    "            #    512,\n",
    "            #    512\n",
    "            #)\n",
    "            #\n",
    "            #res_7 = downsample_7 + block_4_7\n",
    "            \n",
    "            #print(\"res 7:\", res_7)\n",
    "            \n",
    "            conv_w_a = tf.get_variable(\n",
    "                \"conv_w_disc_end_3x3\",\n",
    "                [3, 3, 512, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            conv_out_1 = tf.nn.leaky_relu(\n",
    "                tf.nn.conv2d(res_6, conv_w_a, padding=\"SAME\"),\n",
    "                alpha=0.2\n",
    "            )\n",
    "            \n",
    "            conv_w_b = tf.get_variable(\n",
    "                \"conv_w_disc_end_4x4\",\n",
    "                [4, 4, 512, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            conv_out_2 = tf.nn.leaky_relu(\n",
    "                tf.nn.conv2d(conv_out_1, conv_w_b, padding=\"VALID\"),\n",
    "                alpha=0.2\n",
    "            )\n",
    "            \n",
    "            batch_size = tf.shape(conv_out_2)[0]\n",
    "            \n",
    "            conv_outputs_flat = tf.reshape(conv_out_2, shape=[batch_size, -1])\n",
    "            \n",
    "            W_fc = tf.get_variable(\n",
    "                \"fully_connected_W_disc_end\",\n",
    "                [512, 1],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            b_fc = tf.get_variable(\n",
    "                \"fully_connected_b_disc_end\",\n",
    "                [1],\n",
    "                initializer=tf.initializers.random_normal\n",
    "            )\n",
    "            \n",
    "            disc_out = tf.nn.leaky_relu(\n",
    "                tf.matmul(conv_outputs_flat, W_fc) + b_fc\n",
    "            )\n",
    "            \n",
    "            return disc_out\n",
    "            \n",
    "    def discriminatorBlock(self, V_in, num_input_channels, num_output_channels, downsample=True):\n",
    "        # V_in        --> [batch_size, height, width, num_input_channels]\n",
    "        # latent_w    --> [batch_size, 512]\n",
    "        #    num_input_channels  = number of input feature maps\n",
    "        #    num_output_channels = number of output feature maps\n",
    "        self.num_discriminator_blocks += 1\n",
    "        \n",
    "        conv_weight_a = tf.get_variable(\n",
    "            \"conv_w_disc_a_\" + str(self.num_discriminator_blocks),\n",
    "            [3, 3, num_input_channels, num_input_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        V_out_a = tf.nn.leaky_relu(\n",
    "            tf.nn.conv2d(V_in, conv_weight_a, padding=\"SAME\"),\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        conv_weight_b = tf.get_variable(\n",
    "            \"conv_w_disc_b_\" + str(self.num_discriminator_blocks),\n",
    "            [3, 3, num_input_channels, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        V_out_b = tf.nn.leaky_relu(\n",
    "            tf.nn.conv2d(V_out_a, conv_weight_b, padding=\"SAME\"),\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        V_out = self.downsample(V_out_b, num_output_channels, num_output_channels)\n",
    "        \n",
    "        return V_out\n",
    "            \n",
    "    def styleBlock(self, V_in, latent_w, num_input_channels, num_output_channels, fm_dimension, upsample=False):\n",
    "        # V_in        --> [batch_size, height, width, num_input_channels]\n",
    "        # latent_w    --> [batch_size, 512]\n",
    "        #    num_input_channels  = number of input feature maps\n",
    "        #    num_output_channels = number of output feature maps\n",
    "        self.num_style_blocks += 1\n",
    "        \n",
    "        if upsample:\n",
    "            V_in = self.upsample(V_in)\n",
    "        \n",
    "        A = tf.get_variable(\n",
    "            \"A_style\" + str(self.num_style_blocks),\n",
    "            [512, num_input_channels],\n",
    "            #[512, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        conv_weight = tf.get_variable(\n",
    "            \"conv_w_style\" + str(self.num_style_blocks),\n",
    "            [3, 3, num_input_channels, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        conv_bias = tf.get_variable(\n",
    "            \"conv_b_style\" + str(self.num_style_blocks),\n",
    "            [1, fm_dimension, fm_dimension, num_output_channels],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        \n",
    "        # Affine transformation of latent space vector.\n",
    "        scale = tf.matmul(latent_w, A)\n",
    "        \n",
    "        # Scale input feature map acros input channels by the affine transformation\n",
    "        # of the latent space input.\n",
    "        print(\"##########\")\n",
    "        print(\"scale input feature map\")\n",
    "        print(\"scale\", scale)\n",
    "        print(\"V_in\", V_in)\n",
    "        V_in_scaled = tf.einsum(\"bi,bhwi->bhwi\", scale, V_in)\n",
    "        \n",
    "        V_out = tf.nn.conv2d(V_in_scaled, conv_weight, padding=\"SAME\")\n",
    "        print(\"V_out:\", V_out)\n",
    "        # This increases the number of weights by a factor of batch_size,\n",
    "        # which is weird.\n",
    "        #print(\"#########\")\n",
    "        print(\"calculate sigma_j\")\n",
    "        print(\"scale\", scale)\n",
    "        print(\"conv_weight\", conv_weight)\n",
    "        #modul_conv_weight = tf.einsum(\"bc,hwjc->bhwjc\", scale, conv_weight)\n",
    "        modul_conv_weight = tf.einsum(\"bj,hwjc->bhwjc\", scale, conv_weight)\n",
    "        sigma_j = tf.sqrt(tf.reduce_sum(tf.square(modul_conv_weight), axis=[1, 2, 3]) + 1e-6)\n",
    "        \n",
    "        #print(\"#############\")\n",
    "        print(\"calculate output\")\n",
    "        print(\"V_in_scaled\", V_in_scaled)\n",
    "        print(\"sigma_j\", sigma_j)\n",
    "        # Need to add biases and broadcast noise.\n",
    "        V_out_scaled = tf.nn.leaky_relu(\n",
    "            tf.einsum(\"bhwj,bj->bhwj\", V_out, sigma_j) + conv_bias,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "        return V_out_scaled\n",
    "    \n",
    "    def upsample(self, V_in):\n",
    "        # Tested with the channel dimension.\n",
    "        fm_size = tf.shape(V_in)\n",
    "        batch_size = fm_size[0]\n",
    "        h = fm_size[1]\n",
    "        w = fm_size[2]\n",
    "        c = fm_size[3]\n",
    "        V_in_a = tf.concat([V_in, V_in,], axis=2)\n",
    "        V_in_b = tf.reshape(V_in_a, [batch_size, 2*h, w, c])\n",
    "\n",
    "        V_in_c = tf.transpose(V_in_b, perm=[0, 2, 1, 3])\n",
    "        V_in_d = tf.concat([V_in_c, V_in_c], axis=2)\n",
    "        V_out = tf.transpose(tf.reshape(V_in_d, [batch_size, 2*h, 2*w, c]), perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return V_out\n",
    "    \n",
    "    def downsample(self, V_in, input_channels, output_channels=None):\n",
    "        self.num_downsamples += 1\n",
    "        \n",
    "        if output_channels is None:\n",
    "            output_channels = 2*input_channels\n",
    "        \n",
    "        channel_increase = tf.get_variable(\n",
    "            \"channel_increaser\" + str(self.num_downsamples),\n",
    "            [1, 1, input_channels, output_channels]\n",
    "        )\n",
    "        \n",
    "        V_larger = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, channel_increase, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        V_out = tf.nn.max_pool2d(V_larger, ksize=2, strides=2, padding=\"VALID\")\n",
    "        return V_out\n",
    "    \n",
    "    def toRgb(self, V_in, h, w, c):\n",
    "        '''\n",
    "        Convert an NxNxC output block to an RGB image with dimensions\n",
    "        NxNx3.\n",
    "        '''\n",
    "        \n",
    "        self.num_to_rgbs += 1\n",
    "\n",
    "        to_rgb = tf.get_variable(\n",
    "            \"to_rgb\" + str(self.num_to_rgbs),\n",
    "            [h, w, c, 3],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        print(\"###############\")\n",
    "        print(\"V_in:\", V_in)\n",
    "        print(\"to_rgb:\", to_rgb)\n",
    "        rgb_out = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, to_rgb, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        return rgb_out\n",
    "    \n",
    "    def fromRgb(self, V_in, h, w, c):\n",
    "        '''\n",
    "        Convert an NxNx3 output block to an feature map with dimensions\n",
    "        NxNxC.\n",
    "        '''\n",
    "        \n",
    "        self.num_from_rgbs += 1\n",
    "        \n",
    "        from_rgb = tf.get_variable(\n",
    "            \"from_rgb\" + str(self.num_from_rgbs),\n",
    "            [h, w, 3, c],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        \n",
    "        feature_map_out = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, from_rgb, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        return feature_map_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiled constant input: Tensor(\"generator/Tile:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/Tile:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style1:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum/transpose_2:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt:0\", shape=(64, 512), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb1:0' shape=(4, 4, 512, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_1:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_1:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_2:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_1:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style2:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_3/transpose_2:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_1:0\", shape=(64, 512), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_2:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_1:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_3:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_2:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style3:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_6/transpose_2:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_2:0\", shape=(64, 512), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_2:0\", shape=(64, 8, 8, 512), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb2:0' shape=(8, 8, 512, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_3:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_5:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_5:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_3:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style4:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_9/transpose_2:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_3:0\", shape=(64, 512), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_4:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_3:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_6:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_4:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style5:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_12/transpose_2:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_4:0\", shape=(64, 512), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_4:0\", shape=(64, 16, 16, 512), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb3:0' shape=(16, 16, 512, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_5:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_9:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_8:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_5:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style6:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_15/transpose_2:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_5:0\", shape=(64, 512), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_6:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_5:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_9:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_6:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style7:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_18/transpose_2:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_6:0\", shape=(64, 512), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_6:0\", shape=(64, 32, 32, 512), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb4:0' shape=(32, 32, 512, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_7:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_13:0\", shape=(64, 64, 64, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_11:0\", shape=(64, 64, 64, 512), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_7:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style8:0' shape=(3, 3, 512, 512) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_21/transpose_2:0\", shape=(64, 64, 64, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_7:0\", shape=(64, 512), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_8:0\", shape=(64, 512), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_7:0\", shape=(64, 64, 64, 512), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_12:0\", shape=(64, 64, 64, 256), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_8:0\", shape=(64, 512), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style9:0' shape=(3, 3, 512, 256) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_24/transpose_2:0\", shape=(64, 64, 64, 512), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_8:0\", shape=(64, 256), dtype=float32)\n",
      "block_64_2: Tensor(\"generator/LeakyRelu_8:0\", shape=(64, 64, 64, 256), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_8:0\", shape=(64, 64, 64, 256), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb5:0' shape=(64, 64, 256, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_9:0\", shape=(64, 256), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_17:0\", shape=(64, 128, 128, 256), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_14:0\", shape=(64, 128, 128, 256), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_9:0\", shape=(64, 256), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style10:0' shape=(3, 3, 256, 256) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_27/transpose_2:0\", shape=(64, 128, 128, 256), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_9:0\", shape=(64, 256), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_10:0\", shape=(64, 256), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_9:0\", shape=(64, 128, 128, 256), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_15:0\", shape=(64, 128, 128, 128), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_10:0\", shape=(64, 256), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style11:0' shape=(3, 3, 256, 128) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_30/transpose_2:0\", shape=(64, 128, 128, 256), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_10:0\", shape=(64, 128), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_10:0\", shape=(64, 128, 128, 128), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb6:0' shape=(128, 128, 128, 3) dtype=float32_ref>\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_11:0\", shape=(64, 128), dtype=float32)\n",
      "V_in Tensor(\"generator/transpose_21:0\", shape=(64, 256, 256, 128), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_17:0\", shape=(64, 256, 256, 128), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_11:0\", shape=(64, 128), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style12:0' shape=(3, 3, 128, 128) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_33/transpose_2:0\", shape=(64, 256, 256, 128), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_11:0\", shape=(64, 128), dtype=float32)\n",
      "##########\n",
      "scale input feature map\n",
      "scale Tensor(\"generator/MatMul_12:0\", shape=(64, 128), dtype=float32)\n",
      "V_in Tensor(\"generator/LeakyRelu_11:0\", shape=(64, 256, 256, 128), dtype=float32)\n",
      "V_out: Tensor(\"generator/Conv2D_18:0\", shape=(64, 256, 256, 64), dtype=float32)\n",
      "calculate sigma_j\n",
      "scale Tensor(\"generator/MatMul_12:0\", shape=(64, 128), dtype=float32)\n",
      "conv_weight <tf.Variable 'generator/conv_w_style13:0' shape=(3, 3, 128, 64) dtype=float32_ref>\n",
      "calculate output\n",
      "V_in_scaled Tensor(\"generator/einsum_36/transpose_2:0\", shape=(64, 256, 256, 128), dtype=float32)\n",
      "sigma_j Tensor(\"generator/Sqrt_12:0\", shape=(64, 64), dtype=float32)\n",
      "###############\n",
      "V_in: Tensor(\"generator/LeakyRelu_12:0\", shape=(64, 256, 256, 64), dtype=float32)\n",
      "to_rgb: <tf.Variable 'generator/to_rgb7:0' shape=(256, 256, 64, 3) dtype=float32_ref>\n",
      "from rgb: Tensor(\"discriminator/Relu:0\", shape=(64, 256, 256, 16), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res 1: Tensor(\"discriminator/add:0\", shape=(64, 128, 128, 32), dtype=float32)\n",
      "res 2: Tensor(\"discriminator/add_1:0\", shape=(64, 64, 64, 64), dtype=float32)\n",
      "from rgb: Tensor(\"discriminator_1/Relu:0\", shape=(?, 256, 256, 16), dtype=float32)\n",
      "res 1: Tensor(\"discriminator_1/add:0\", shape=(?, 128, 128, 32), dtype=float32)\n",
      "res 2: Tensor(\"discriminator_1/add_1:0\", shape=(?, 64, 64, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "s = stylegan(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(\n",
    "    np.array(\n",
    "        [\n",
    "            [[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9]\n",
    "            ],\n",
    "            [[-1,-2,-3],\n",
    "             [-4,-5,-6],\n",
    "             [-7,-8,-9]\n",
    "            ],\n",
    "            [[1.2,2.2,3.2],\n",
    "             [4.2,5.2,6.2],\n",
    "             [7.2,8.2,9.2]\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.   1.   2.   2.   3.   3. ]\n",
      "  [ 1.   1.   2.   2.   3.   3. ]\n",
      "  [ 4.   4.   5.   5.   6.   6. ]\n",
      "  [ 4.   4.   5.   5.   6.   6. ]\n",
      "  [ 7.   7.   8.   8.   9.   9. ]\n",
      "  [ 7.   7.   8.   8.   9.   9. ]]\n",
      "\n",
      " [[-1.  -1.  -2.  -2.  -3.  -3. ]\n",
      "  [-1.  -1.  -2.  -2.  -3.  -3. ]\n",
      "  [-4.  -4.  -5.  -5.  -6.  -6. ]\n",
      "  [-4.  -4.  -5.  -5.  -6.  -6. ]\n",
      "  [-7.  -7.  -8.  -8.  -9.  -9. ]\n",
      "  [-7.  -7.  -8.  -8.  -9.  -9. ]]\n",
      "\n",
      " [[ 1.2  1.2  2.2  2.2  3.2  3.2]\n",
      "  [ 1.2  1.2  2.2  2.2  3.2  3.2]\n",
      "  [ 4.2  4.2  5.2  5.2  6.2  6.2]\n",
      "  [ 4.2  4.2  5.2  5.2  6.2  6.2]\n",
      "  [ 7.2  7.2  8.2  8.2  9.2  9.2]\n",
      "  [ 7.2  7.2  8.2  8.2  9.2  9.2]]]\n",
      "[[[  2.    2.    4.    4.    6.    6. ]\n",
      "  [  2.    2.    4.    4.    6.    6. ]\n",
      "  [  8.    8.   10.   10.   12.   12. ]\n",
      "  [  8.    8.   10.   10.   12.   12. ]\n",
      "  [ 14.   14.   16.   16.   18.   18. ]\n",
      "  [ 14.   14.   16.   16.   18.   18. ]]\n",
      "\n",
      " [[ -2.   -2.   -4.   -4.   -6.   -6. ]\n",
      "  [ -2.   -2.   -4.   -4.   -6.   -6. ]\n",
      "  [ -8.   -8.  -10.  -10.  -12.  -12. ]\n",
      "  [ -8.   -8.  -10.  -10.  -12.  -12. ]\n",
      "  [-14.  -14.  -16.  -16.  -18.  -18. ]\n",
      "  [-14.  -14.  -16.  -16.  -18.  -18. ]]\n",
      "\n",
      " [[  2.4   2.4   4.4   4.4   6.4   6.4]\n",
      "  [  2.4   2.4   4.4   4.4   6.4   6.4]\n",
      "  [  8.4   8.4  10.4  10.4  12.4  12.4]\n",
      "  [  8.4   8.4  10.4  10.4  12.4  12.4]\n",
      "  [ 14.4  14.4  16.4  16.4  18.4  18.4]\n",
      "  [ 14.4  14.4  16.4  16.4  18.4  18.4]]]\n",
      "[[[  3.4    3.4    6.8    6.8   10.2   10.2 ]\n",
      "  [  3.4    3.4    6.8    6.8   10.2   10.2 ]\n",
      "  [ 13.6   13.6   17.    17.    20.4   20.4 ]\n",
      "  [ 13.6   13.6   17.    17.    20.4   20.4 ]\n",
      "  [ 23.8   23.8   27.2   27.2   30.6   30.6 ]\n",
      "  [ 23.8   23.8   27.2   27.2   30.6   30.6 ]]\n",
      "\n",
      " [[ -3.4   -3.4   -6.8   -6.8  -10.2  -10.2 ]\n",
      "  [ -3.4   -3.4   -6.8   -6.8  -10.2  -10.2 ]\n",
      "  [-13.6  -13.6  -17.   -17.   -20.4  -20.4 ]\n",
      "  [-13.6  -13.6  -17.   -17.   -20.4  -20.4 ]\n",
      "  [-23.8  -23.8  -27.2  -27.2  -30.6  -30.6 ]\n",
      "  [-23.8  -23.8  -27.2  -27.2  -30.6  -30.6 ]]\n",
      "\n",
      " [[  4.08   4.08   7.48   7.48  10.88  10.88]\n",
      "  [  4.08   4.08   7.48   7.48  10.88  10.88]\n",
      "  [ 14.28  14.28  17.68  17.68  21.08  21.08]\n",
      "  [ 14.28  14.28  17.68  17.68  21.08  21.08]\n",
      "  [ 24.48  24.48  27.88  27.88  31.28  31.28]\n",
      "  [ 24.48  24.48  27.88  27.88  31.28  31.28]]]\n"
     ]
    }
   ],
   "source": [
    "b = tf.concat([a, a,], axis=2)\n",
    "c = tf.reshape(b, [3,6,3])\n",
    "\n",
    "d = tf.transpose(c, perm=[0, 2, 1])\n",
    "e = tf.concat([d, d], axis=2)\n",
    "f = tf.transpose(tf.reshape(e, [3, 6, 6]), perm=[0, 2, 1])\n",
    "\n",
    "g = tf.stack([a, 2*a, 3.4*a], axis=3)\n",
    "h = tf.concat([g, g], axis=2)\n",
    "i = tf.reshape(h, [3, 6, 3, 3])\n",
    "j = tf.transpose(i, perm=[0, 2, 1, 3])\n",
    "k = tf.concat([j, j], axis=2)\n",
    "l = tf.transpose(tf.reshape(k, [3, 6, 6, 3]), perm=[0, 2, 1, 3])\n",
    "\n",
    "for i in range(3):\n",
    "    print(sess.run(l)[:, :, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1. ,  2. ,  3. ],\n",
       "        [ 1. ,  2. ,  3. ],\n",
       "        [ 4. ,  5. ,  6. ],\n",
       "        [ 4. ,  5. ,  6. ],\n",
       "        [ 7. ,  8. ,  9. ],\n",
       "        [ 7. ,  8. ,  9. ]],\n",
       "\n",
       "       [[-1. , -2. , -3. ],\n",
       "        [-1. , -2. , -3. ],\n",
       "        [-4. , -5. , -6. ],\n",
       "        [-4. , -5. , -6. ],\n",
       "        [-7. , -8. , -9. ],\n",
       "        [-7. , -8. , -9. ]],\n",
       "\n",
       "       [[ 1.2,  2.2,  3.2],\n",
       "        [ 1.2,  2.2,  3.2],\n",
       "        [ 4.2,  5.2,  6.2],\n",
       "        [ 4.2,  5.2,  6.2],\n",
       "        [ 7.2,  8.2,  9.2],\n",
       "        [ 7.2,  8.2,  9.2]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1. ,  1. ,  2. ,  2. ,  3. ,  3. ],\n",
       "        [ 1. ,  1. ,  2. ,  2. ,  3. ,  3. ],\n",
       "        [ 4. ,  4. ,  5. ,  5. ,  6. ,  6. ],\n",
       "        [ 4. ,  4. ,  5. ,  5. ,  6. ,  6. ],\n",
       "        [ 7. ,  7. ,  8. ,  8. ,  9. ,  9. ],\n",
       "        [ 7. ,  7. ,  8. ,  8. ,  9. ,  9. ]],\n",
       "\n",
       "       [[-1. , -1. , -2. , -2. , -3. , -3. ],\n",
       "        [-1. , -1. , -2. , -2. , -3. , -3. ],\n",
       "        [-4. , -4. , -5. , -5. , -6. , -6. ],\n",
       "        [-4. , -4. , -5. , -5. , -6. , -6. ],\n",
       "        [-7. , -7. , -8. , -8. , -9. , -9. ],\n",
       "        [-7. , -7. , -8. , -8. , -9. , -9. ]],\n",
       "\n",
       "       [[ 1.2,  1.2,  2.2,  2.2,  3.2,  3.2],\n",
       "        [ 1.2,  1.2,  2.2,  2.2,  3.2,  3.2],\n",
       "        [ 4.2,  4.2,  5.2,  5.2,  6.2,  6.2],\n",
       "        [ 4.2,  4.2,  5.2,  5.2,  6.2,  6.2],\n",
       "        [ 7.2,  7.2,  8.2,  8.2,  9.2,  9.2],\n",
       "        [ 7.2,  7.2,  8.2,  8.2,  9.2,  9.2]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "reshape(tensor, shape, name=None)\n",
      "    Reshapes a tensor.\n",
      "    \n",
      "    Given `tensor`, this operation returns a tensor that has the same values\n",
      "    as `tensor` with shape `shape`.\n",
      "    \n",
      "    If one component of `shape` is the special value -1, the size of that dimension\n",
      "    is computed so that the total size remains constant.  In particular, a `shape`\n",
      "    of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.\n",
      "    \n",
      "    If `shape` is 1-D or higher, then the operation returns a tensor with shape\n",
      "    `shape` filled with the values of `tensor`. In this case, the number of elements\n",
      "    implied by `shape` must be the same as the number of elements in `tensor`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    # tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    # tensor 't' has shape [9]\n",
      "    reshape(t, [3, 3]) ==> [[1, 2, 3],\n",
      "                            [4, 5, 6],\n",
      "                            [7, 8, 9]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1], [2, 2]],\n",
      "    #                [[3, 3], [4, 4]]]\n",
      "    # tensor 't' has shape [2, 2, 2]\n",
      "    reshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n",
      "                            [3, 3, 4, 4]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1, 1],\n",
      "    #                 [2, 2, 2]],\n",
      "    #                [[3, 3, 3],\n",
      "    #                 [4, 4, 4]],\n",
      "    #                [[5, 5, 5],\n",
      "    #                 [6, 6, 6]]]\n",
      "    # tensor 't' has shape [3, 2, 3]\n",
      "    # pass '[-1]' to flatten 't'\n",
      "    reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
      "    \n",
      "    # -1 can also be used to infer the shape\n",
      "    \n",
      "    # -1 is inferred to be 9:\n",
      "    reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 2:\n",
      "    reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 3:\n",
      "    reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
      "                                  [2, 2, 2],\n",
      "                                  [3, 3, 3]],\n",
      "                                 [[4, 4, 4],\n",
      "                                  [5, 5, 5],\n",
      "                                  [6, 6, 6]]]\n",
      "    \n",
      "    # tensor 't' is [7]\n",
      "    # shape `[]` reshapes to a scalar\n",
      "    reshape(t, []) ==> 7\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        Defines the shape of the output tensor.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tile in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "tile(input, multiples, name=None)\n",
      "    Constructs a tensor by tiling a given tensor.\n",
      "    \n",
      "    This operation creates a new tensor by replicating `input` `multiples` times.\n",
      "    The output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\n",
      "    and the values of `input` are replicated `multiples[i]` times along the 'i'th\n",
      "    dimension. For example, tiling `[a b c d]` by `[2]` produces\n",
      "    `[a b c d a b c d]`.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. 1-D or higher.\n",
      "      multiples: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        1-D. Length must be the same as the number of dimensions in `input`\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose(a, perm=None, name='transpose', conjugate=False)\n",
      "    Transposes `a`.\n",
      "    \n",
      "    Permutes the dimensions according to `perm`.\n",
      "    \n",
      "    The returned tensor's dimension i will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is\n",
      "    the rank of the input tensor. Hence by default, this operation performs a\n",
      "    regular matrix transpose on 2-D input Tensors. If conjugate is True and\n",
      "    `a.dtype` is either `complex64` or `complex128` then the values of `a`\n",
      "    are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    tf.transpose(x)  # [[1, 4]\n",
      "                     #  [2, 5]\n",
      "                     #  [3, 6]]\n",
      "    \n",
      "    # Equivalently\n",
      "    tf.transpose(x, perm=[1, 0])  # [[1, 4]\n",
      "                                  #  [2, 5]\n",
      "                                  #  [3, 6]]\n",
      "    \n",
      "    # If x is complex, setting conjugate=True gives the conjugate transpose\n",
      "    x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "                     [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    tf.transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],\n",
      "                                     #  [2 - 2j, 5 - 5j],\n",
      "                                     #  [3 - 3j, 6 - 6j]]\n",
      "    \n",
      "    # 'perm' is more useful for n-dimensional tensors, for n > 2\n",
      "    x = tf.constant([[[ 1,  2,  3],\n",
      "                      [ 4,  5,  6]],\n",
      "                     [[ 7,  8,  9],\n",
      "                      [10, 11, 12]]])\n",
      "    \n",
      "    # Take the transpose of the matrices in dimension-0\n",
      "    # (this common operation has a shorthand `linalg.matrix_transpose`)\n",
      "    tf.transpose(x, perm=[0, 2, 1])  # [[[1,  4],\n",
      "                                     #   [2,  5],\n",
      "                                     #   [3,  6]],\n",
      "                                     #  [[7, 10],\n",
      "                                     #   [8, 11],\n",
      "                                     #   [9, 12]]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.\n",
      "      name: A name for the operation (optional).\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "stack(values, axis=0, name='stack')\n",
      "    Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n",
      "    \n",
      "    Packs the list of tensors in `values` into a tensor with rank one higher than\n",
      "    each tensor in `values`, by packing them along the `axis` dimension.\n",
      "    Given a list of length `N` of tensors of shape `(A, B, C)`;\n",
      "    \n",
      "    if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n",
      "    if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n",
      "    Etc.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    x = tf.constant([1, 4])\n",
      "    y = tf.constant([2, 5])\n",
      "    z = tf.constant([3, 6])\n",
      "    tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)\n",
      "    tf.stack([x, y, z], axis=1)  # [[1, 2, 3], [4, 5, 6]]\n",
      "    ```\n",
      "    \n",
      "    This is the opposite of unstack.  The numpy equivalent is\n",
      "    \n",
      "    ```python\n",
      "    tf.stack([x, y, z]) = np.stack([x, y, z])\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      values: A list of `Tensor` objects with the same shape and type.\n",
      "      axis: An `int`. The axis to stack along. Defaults to the first dimension.\n",
      "        Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n",
      "      name: A name for this operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      output: A stacked `Tensor` with the same type as `values`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `axis` is out of the range [-(R+1), R+1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function max_pool2d in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "max_pool2d(input, ksize, strides, padding, data_format='NHWC', name=None)\n",
      "    Performs the max pooling on the input.\n",
      "    \n",
      "    Args:\n",
      "      input: A 4-D `Tensor` of the format specified by `data_format`.\n",
      "      ksize: An int or list of `ints` that has length `1`, `2` or `4`. The size of\n",
      "        the window for each dimension of the input tensor.\n",
      "      strides: An int or list of `ints` that has length `1`, `2` or `4`. The\n",
      "        stride of the sliding window for each dimension of the input tensor.\n",
      "      padding: A string, either `'VALID'` or `'SAME'`. The padding algorithm. See\n",
      "        the \"returns\" section of `tf.nn.convolution` for details.\n",
      "      data_format: A string. 'NHWC', 'NCHW' and 'NCHW_VECT_C' are supported.\n",
      "      name: Optional name for the operation.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` of format specified by `data_format`.\n",
      "      The max pooled output tensor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.max_pool2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function conv2d in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "conv2d(input, filter=None, strides=None, padding=None, use_cudnn_on_gpu=True, data_format='NHWC', dilations=[1, 1, 1, 1], name=None, filters=None)\n",
      "    Computes a 2-D convolution given 4-D `input` and `filter` tensors.\n",
      "    \n",
      "    Given an input tensor of shape `[batch, in_height, in_width, in_channels]`\n",
      "    and a filter / kernel tensor of shape\n",
      "    `[filter_height, filter_width, in_channels, out_channels]`, this op\n",
      "    performs the following:\n",
      "    \n",
      "    1. Flattens the filter to a 2-D matrix with shape\n",
      "       `[filter_height * filter_width * in_channels, output_channels]`.\n",
      "    2. Extracts image patches from the input tensor to form a *virtual*\n",
      "       tensor of shape `[batch, out_height, out_width,\n",
      "       filter_height * filter_width * in_channels]`.\n",
      "    3. For each patch, right-multiplies the filter matrix and the image patch\n",
      "       vector.\n",
      "    \n",
      "    In detail, with the default NHWC format,\n",
      "    \n",
      "        output[b, i, j, k] =\n",
      "            sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q]\n",
      "                            * filter[di, dj, q, k]\n",
      "    \n",
      "    Must have `strides[0] = strides[3] = 1`.  For the most common case of the same\n",
      "    horizontal and vertices strides, `strides = [1, stride, stride, 1]`.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. Must be one of the following types:\n",
      "        `half`, `bfloat16`, `float32`, `float64`.\n",
      "        A 4-D tensor. The dimension order is interpreted according to the value\n",
      "        of `data_format`, see below for details.\n",
      "      filter: A `Tensor`. Must have the same type as `input`.\n",
      "        A 4-D tensor of shape\n",
      "        `[filter_height, filter_width, in_channels, out_channels]`\n",
      "      strides: An int or list of `ints` that has length `1`, `2` or `4`.  The\n",
      "        stride of the sliding window for each dimension of `input`. If a single\n",
      "        value is given it is replicated in the `H` and `W` dimension. By default\n",
      "        the `N` and `C` dimensions are set to 1. The dimension order is determined\n",
      "        by the value of `data_format`, see below for details.\n",
      "      padding: Either the `string` `\"SAME\"` or `\"VALID\"` indicating the type of\n",
      "        padding algorithm to use, or a list indicating the explicit paddings at\n",
      "        the start and end of each dimension. When explicit padding is used and\n",
      "        data_format is `\"NHWC\"`, this should be in the form `[[0, 0], [pad_top,\n",
      "        pad_bottom], [pad_left, pad_right], [0, 0]]`. When explicit padding used\n",
      "        and data_format is `\"NCHW\"`, this should be in the form `[[0, 0], [0, 0],\n",
      "        [pad_top, pad_bottom], [pad_left, pad_right]]`.\n",
      "      use_cudnn_on_gpu: An optional `bool`. Defaults to `True`.\n",
      "      data_format: An optional `string` from: `\"NHWC\", \"NCHW\"`.\n",
      "        Defaults to `\"NHWC\"`.\n",
      "        Specify the data format of the input and output data. With the\n",
      "        default format \"NHWC\", the data is stored in the order of:\n",
      "            [batch, height, width, channels].\n",
      "        Alternatively, the format could be \"NCHW\", the data storage order of:\n",
      "            [batch, channels, height, width].\n",
      "      dilations: An int or list of `ints` that has length `1`, `2` or `4`,\n",
      "        defaults to 1. The dilation factor for each dimension of`input`. If a\n",
      "        single value is given it is replicated in the `H` and `W` dimension. By\n",
      "        default the `N` and `C` dimensions are set to 1. If set to k > 1, there\n",
      "        will be k-1 skipped cells between each filter element on that dimension.\n",
      "        The dimension order is determined by the value of `data_format`, see above\n",
      "        for details. Dilations in the batch and depth dimensions if a 4-d tensor\n",
      "        must be 1.\n",
      "      name: A name for the operation (optional).\n",
      "      filters: Alias for filter.\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.conv2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function concat in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "concat(values, axis, name='concat')\n",
      "    Concatenates tensors along one dimension.\n",
      "    \n",
      "    Concatenates the list of tensors `values` along dimension `axis`.  If\n",
      "    `values[i].shape = [D0, D1, ... Daxis(i), ...Dn]`, the concatenated\n",
      "    result has shape\n",
      "    \n",
      "        [D0, D1, ... Raxis, ...Dn]\n",
      "    \n",
      "    where\n",
      "    \n",
      "        Raxis = sum(Daxis(i))\n",
      "    \n",
      "    That is, the data from the input tensors is joined along the `axis`\n",
      "    dimension.\n",
      "    \n",
      "    The number of dimensions of the input tensors must match, and all dimensions\n",
      "    except `axis` must be equal.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    t1 = [[1, 2, 3], [4, 5, 6]]\n",
      "    t2 = [[7, 8, 9], [10, 11, 12]]\n",
      "    tf.concat([t1, t2], 0)  # [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
      "    tf.concat([t1, t2], 1)  # [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]]\n",
      "    \n",
      "    # tensor t3 with shape [2, 3]\n",
      "    # tensor t4 with shape [2, 3]\n",
      "    tf.shape(tf.concat([t3, t4], 0))  # [4, 3]\n",
      "    tf.shape(tf.concat([t3, t4], 1))  # [2, 6]\n",
      "    ```\n",
      "    As in Python, the `axis` could also be negative numbers. Negative `axis`\n",
      "    are interpreted as counting from the end of the rank, i.e.,\n",
      "     `axis + rank(values)`-th dimension.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]\n",
      "    t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]\n",
      "    tf.concat([t1, t2], -1)\n",
      "    ```\n",
      "    \n",
      "    would produce:\n",
      "    \n",
      "    ```python\n",
      "    [[[ 1,  2,  7,  4],\n",
      "      [ 2,  3,  8,  4]],\n",
      "    \n",
      "     [[ 4,  4,  2, 10],\n",
      "      [ 5,  3, 15, 11]]]\n",
      "    ```\n",
      "    \n",
      "    Note: If you are concatenating along a new axis consider using stack.\n",
      "    E.g.\n",
      "    \n",
      "    ```python\n",
      "    tf.concat([tf.expand_dims(t, axis) for t in tensors], axis)\n",
      "    ```\n",
      "    \n",
      "    can be rewritten as\n",
      "    \n",
      "    ```python\n",
      "    tf.stack(tensors, axis=axis)\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      values: A list of `Tensor` objects or a single `Tensor`.\n",
      "      axis: 0-D `int32` `Tensor`.  Dimension along which to concatenate. Must be\n",
      "        in the range `[-rank(values), rank(values))`. As in Python, indexing for\n",
      "        axis is 0-based. Positive axis in the rage of `[0, rank(values))` refers\n",
      "        to `axis`-th dimension. And negative axis refers to `axis +\n",
      "        rank(values)`-th dimension.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` resulting from concatenation of the input tensors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.tile(tf.expand_dims(tf.constant([2,]), axis=1), [tf.constant(64), 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_variable in module tensorflow.python.ops.variable_scope:\n",
      "\n",
      "get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None, constraint=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>)\n",
      "    Gets an existing variable with these parameters or create a new one.\n",
      "    \n",
      "    This function prefixes the name with the current variable scope\n",
      "    and performs reuse checks. See the\n",
      "    [Variable Scope How To](https://tensorflow.org/guide/variables)\n",
      "    for an extensive description of how reusing works. Here is a basic example:\n",
      "    \n",
      "    ```python\n",
      "    def foo():\n",
      "      with tf.variable_scope(\"foo\", reuse=tf.AUTO_REUSE):\n",
      "        v = tf.get_variable(\"v\", [1])\n",
      "      return v\n",
      "    \n",
      "    v1 = foo()  # Creates v.\n",
      "    v2 = foo()  # Gets the same, existing v.\n",
      "    assert v1 == v2\n",
      "    ```\n",
      "    \n",
      "    If initializer is `None` (the default), the default initializer passed in\n",
      "    the variable scope will be used. If that one is `None` too, a\n",
      "    `glorot_uniform_initializer` will be used. The initializer can also be\n",
      "    a Tensor, in which case the variable is initialized to this value and shape.\n",
      "    \n",
      "    Similarly, if the regularizer is `None` (the default), the default regularizer\n",
      "    passed in the variable scope will be used (if that is `None` too,\n",
      "    then by default no regularization is performed).\n",
      "    \n",
      "    If a partitioner is provided, a `PartitionedVariable` is returned.\n",
      "    Accessing this object as a `Tensor` returns the shards concatenated along\n",
      "    the partition axis.\n",
      "    \n",
      "    Some useful partitioners are available.  See, e.g.,\n",
      "    `variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n",
      "    \n",
      "    Args:\n",
      "      name: The name of the new or existing variable.\n",
      "      shape: Shape of the new or existing variable.\n",
      "      dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).\n",
      "      initializer: Initializer for the variable if one is created. Can either be\n",
      "        an initializer object or a Tensor. If it's a Tensor, its shape must be known\n",
      "        unless validate_shape is False.\n",
      "      regularizer: A (Tensor -> Tensor or None) function; the result of\n",
      "        applying it on a newly created variable will be added to the collection\n",
      "        `tf.GraphKeys.REGULARIZATION_LOSSES` and can be used for regularization.\n",
      "      trainable: If `True` also add the variable to the graph collection\n",
      "        `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n",
      "      collections: List of graph collections keys to add the Variable to.\n",
      "        Defaults to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).\n",
      "      caching_device: Optional device string or function describing where the\n",
      "        Variable should be cached for reading.  Defaults to the Variable's\n",
      "        device.  If not `None`, caches on another device.  Typical use is to\n",
      "        cache on the device where the Ops using the Variable reside, to\n",
      "        deduplicate copying through `Switch` and other conditional statements.\n",
      "      partitioner: Optional callable that accepts a fully defined `TensorShape`\n",
      "        and `dtype` of the Variable to be created, and returns a list of\n",
      "        partitions for each axis (currently only one axis can be partitioned).\n",
      "      validate_shape: If False, allows the variable to be initialized with a\n",
      "          value of unknown shape. If True, the default, the shape of initial_value\n",
      "          must be known. For this to be used the initializer must be a Tensor and\n",
      "          not an initializer object.\n",
      "      use_resource: If False, creates a regular Variable. If true, creates an\n",
      "        experimental ResourceVariable instead with well-defined semantics.\n",
      "        Defaults to False (will later change to True). When eager execution is\n",
      "        enabled this argument is always forced to be True.\n",
      "      custom_getter: Callable that takes as a first argument the true getter, and\n",
      "        allows overwriting the internal get_variable method.\n",
      "        The signature of `custom_getter` should match that of this method,\n",
      "        but the most future-proof version will allow for changes:\n",
      "        `def custom_getter(getter, *args, **kwargs)`.  Direct access to\n",
      "        all `get_variable` parameters is also allowed:\n",
      "        `def custom_getter(getter, name, *args, **kwargs)`.  A simple identity\n",
      "        custom getter that simply creates variables with modified names is:\n",
      "        ```python\n",
      "        def custom_getter(getter, name, *args, **kwargs):\n",
      "          return getter(name + '_suffix', *args, **kwargs)\n",
      "        ```\n",
      "      constraint: An optional projection function to be applied to the variable\n",
      "        after being updated by an `Optimizer` (e.g. used to implement norm\n",
      "        constraints or value constraints for layer weights). The function must\n",
      "        take as input the unprojected Tensor representing the value of the\n",
      "        variable and return the Tensor for the projected value\n",
      "        (which must have the same shape). Constraints are not safe to\n",
      "        use when doing asynchronous distributed training.\n",
      "      synchronization: Indicates when a distributed a variable will be\n",
      "        aggregated. Accepted values are constants defined in the class\n",
      "        `tf.VariableSynchronization`. By default the synchronization is set to\n",
      "        `AUTO` and the current `DistributionStrategy` chooses\n",
      "        when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      "        `trainable` must not be set to `True`.\n",
      "      aggregation: Indicates how a distributed variable will be aggregated.\n",
      "        Accepted values are constants defined in the class\n",
      "        `tf.VariableAggregation`.\n",
      "    \n",
      "    Returns:\n",
      "      The created or existing `Variable` (or `PartitionedVariable`, if a\n",
      "      partitioner was used).\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: when creating a new variable and shape is not declared,\n",
      "        when violating reuse during variable creation, or when `initializer` dtype\n",
      "        and `dtype` don't match. Reuse is set inside `variable_scope`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.get_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
