{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stylegan(object):\n",
    "    def __init__(self, session, batch_size=64, output_resolution=256, gamma=10):\n",
    "        self.sess = session\n",
    "        self.output_resolution = output_resolution\n",
    "        self.num_style_blocks = 0\n",
    "        self.num_discriminator_blocks = 0\n",
    "        self.num_to_rgbs = 0\n",
    "        self.num_from_rgbs = 0\n",
    "        self.num_downsamples = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.conv_weights = []\n",
    "        self.w_transforms = []\n",
    "        \n",
    "        self.latent_z = tf.random.normal(\n",
    "            shape=[self.batch_size,512],\n",
    "            stddev=1.0\n",
    "        )\n",
    "\n",
    "        self.latent_w = self.latentZMapper(self.latent_z)\n",
    "        \n",
    "        self.true_images_ph = tf.placeholder(\n",
    "            shape=[None, 256, 256, 3],\n",
    "            dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        self.generator_out = self.evalGenerator(self.latent_w)\n",
    "        \n",
    "        e = tf.random.uniform(\n",
    "            shape=[self.batch_size], minval=0., maxval=1., dtype=tf.float32\n",
    "        )\n",
    "        \n",
    "        print(self.generator_out)\n",
    "        \n",
    "        self.interp_images = tf.add(\n",
    "            tf.einsum(\"b,bhwc->bhwc\", e, self.generator_out),\n",
    "            tf.einsum(\"b,bhwc->bhwc\", (1. - e), self.true_images_ph)\n",
    "        )\n",
    "        \n",
    "        self.disc_of_gen_out = self.evalDiscriminator(self.generator_out)\n",
    "        self.disc_of_truth_out = self.evalDiscriminator(self.true_images_ph)\n",
    "        self.disc_of_interp_out = self.evalDiscriminator(self.interp_images)\n",
    "        \n",
    "        disc_grad_truth = tf.gradients(\n",
    "            self.disc_of_truth_out,\n",
    "            [self.true_images_ph]\n",
    "        )[0]\n",
    "        \n",
    "        disc_grad_interp = tf.gradients(\n",
    "            self.disc_of_interp_out,\n",
    "            [self.interp_images]\n",
    "        )[0]\n",
    "        \n",
    "        self.r1_reg = (self.gamma/2)*tf.reduce_mean(\n",
    "            tf.square(disc_grad_truth)\n",
    "        )\n",
    "        \n",
    "        self.lipschitz_penalty = tf.reduce_mean(\n",
    "            tf.square(\n",
    "                tf.sqrt(\n",
    "                    tf.reduce_sum(disc_grad_interp*disc_grad_interp, axis=[1, 2, 3])\n",
    "                ) - 1.\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.wgan_loss = tf.reduce_mean(self.disc_of_gen_out - self.disc_of_truth_out)\n",
    "        \n",
    "        self.disc_loss = self.wgan_loss + self.lipschitz_penalty\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=0.0001, beta1=0, beta2=0.9).minimize(self.disc_loss)\n",
    "        \n",
    "        #random_images = tf.random.normal(mean=127, stddev=10, shape=[64, 256, 256, 3])\n",
    "        \n",
    "        #gen_grad = \n",
    "        \n",
    "    def latentZMapper(self, Z_in, depth=8):\n",
    "        result = None\n",
    "        for i in range(depth):\n",
    "            W = tf.get_variable(\n",
    "                \"W_mapper_\" + str(i),\n",
    "                [512, 512],\n",
    "                initializer=tf.initializers.random_normal(stddev=0.3)\n",
    "            )\n",
    "            b = tf.get_variable(\n",
    "                \"b_mapper_\" + str(i),\n",
    "                [512,],\n",
    "                initializer=tf.initializers.random_normal(stddev=0.3)\n",
    "            )\n",
    "            \n",
    "            if i == 0:\n",
    "                result = tf.nn.relu(tf.matmul(Z_in, W) + b)\n",
    "            else:\n",
    "                result = tf.nn.relu(tf.matmul(result, W) + b)\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    def evalGenerator(self, W_in):\n",
    "        with tf.variable_scope(\"generator\", reuse=tf.AUTO_REUSE) as vs:\n",
    "            self.constant_input = tf.get_variable(\n",
    "                \"c_1\",\n",
    "                [4, 4, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            batch_size = tf.shape(W_in)[0]\n",
    "            \n",
    "            tiled_constant_input = tf.tile(\n",
    "                tf.expand_dims(\n",
    "                    self.constant_input, axis=0\n",
    "                ),\n",
    "                [batch_size, 1, 1, 1]\n",
    "            )\n",
    "            \n",
    "            print(\"tiled constant input:\", tiled_constant_input)\n",
    "            \n",
    "            block_4_2 = self.styleBlock(\n",
    "                tiled_constant_input,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #1\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=4\n",
    "            )\n",
    "            \n",
    "            to_rgb_1 = self.toRgb(block_4_2, 4, 4, 512)\n",
    "            \n",
    "            block_8_1 = self.styleBlock(\n",
    "                block_4_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #2\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=8,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_8_2 = self.styleBlock(\n",
    "                block_8_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #3\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=8\n",
    "            )\n",
    "            \n",
    "            to_rgb_2 = self.toRgb(block_8_2, 8, 8, 512) + self.upsample(to_rgb_1)\n",
    "            \n",
    "            block_16_1 = self.styleBlock(\n",
    "                block_8_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #4\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=16,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_16_2 = self.styleBlock(\n",
    "                block_16_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #5\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=16,\n",
    "            )\n",
    "            \n",
    "            to_rgb_3 = self.toRgb(block_16_2, 16, 16, 512) + self.upsample(to_rgb_2)\n",
    "            \n",
    "            block_32_1 = self.styleBlock(\n",
    "                block_16_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #6\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=32,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_32_2 = self.styleBlock(\n",
    "                block_32_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #7\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=32,\n",
    "            )\n",
    "            \n",
    "            to_rgb_4 = self.toRgb(block_32_2, 32, 32, 512) + self.upsample(to_rgb_3)\n",
    "            \n",
    "            block_64_1 = self.styleBlock(\n",
    "                block_32_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                #8\n",
    "                num_output_channels=512,\n",
    "                fm_dimension=64,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_64_2 = self.styleBlock(\n",
    "                block_64_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=256,\n",
    "                fm_dimension=64,\n",
    "            )\n",
    "            print(\"block_64_2:\", block_64_2)\n",
    "            to_rgb_5 = self.toRgb(block_64_2, 64, 64, 256) + self.upsample(to_rgb_4)\n",
    "            \n",
    "            block_128_1 = self.styleBlock(\n",
    "                block_64_2,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=256,\n",
    "                fm_dimension=128,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_128_2 = self.styleBlock(\n",
    "                block_128_1,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=128,\n",
    "                fm_dimension=128,\n",
    "            )\n",
    "            \n",
    "            to_rgb_6 = self.toRgb(block_128_2, 128, 128, 128) + self.upsample(to_rgb_5)\n",
    "            \n",
    "            block_256_1 = self.styleBlock(\n",
    "                block_128_2,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=128,\n",
    "                fm_dimension=256,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_256_2 = self.styleBlock(\n",
    "                block_256_1,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=64,\n",
    "                fm_dimension=256\n",
    "            )\n",
    "            \n",
    "            to_rgb_7 = self.toRgb(block_256_2, 256, 256, 64) + self.upsample(to_rgb_6)\n",
    "            \n",
    "            return to_rgb_7\n",
    "            \n",
    "    def evalDiscriminator(self, rgb_in):\n",
    "        with tf.variable_scope(\"discriminator\", reuse=tf.AUTO_REUSE) as vs:\n",
    "            from_rgb_1 = self.fromRgb(rgb_in, 256, 256, 16)\n",
    "            \n",
    "            print(\"from rgb:\", from_rgb_1)\n",
    "            \n",
    "            downsample_1 = self.downsample(from_rgb_1, 16)\n",
    "            block_128_1 = self.discriminatorBlock(\n",
    "                from_rgb_1,\n",
    "                16,\n",
    "                32\n",
    "            )\n",
    "            \n",
    "            res_1 = downsample_1 + block_128_1\n",
    "            print(\"res 1:\", res_1)\n",
    "            downsample_2 = self.downsample(res_1, 32)\n",
    "            block_64_2 = self.discriminatorBlock(\n",
    "                res_1,\n",
    "                32,\n",
    "                64\n",
    "            )\n",
    "            \n",
    "            res_2 = downsample_2 + block_64_2\n",
    "            \n",
    "            print(\"res 2:\", res_2)\n",
    "            \n",
    "            downsample_3 = self.downsample(res_2, 64)\n",
    "            block_32_3 = self.discriminatorBlock(\n",
    "                res_2,\n",
    "                64,\n",
    "                128\n",
    "            )\n",
    "            \n",
    "            res_3 = downsample_3 + block_32_3\n",
    "            \n",
    "            downsample_4 = self.downsample(res_3, 128)\n",
    "            block_16_4 = self.discriminatorBlock(\n",
    "                res_3,\n",
    "                128,\n",
    "                256\n",
    "            )\n",
    "            \n",
    "            res_4 = downsample_4 + block_16_4\n",
    "            \n",
    "            downsample_5 = self.downsample(res_4, 256)\n",
    "            block_8_5 = self.discriminatorBlock(\n",
    "                res_4,\n",
    "                256,\n",
    "                512\n",
    "            )\n",
    "            \n",
    "            res_5 = downsample_5 + block_8_5\n",
    "            \n",
    "            downsample_6 = self.downsample(res_5, 512, 512)\n",
    "            block_4_6 = self.discriminatorBlock(\n",
    "                res_5,\n",
    "                512,\n",
    "                512\n",
    "            )\n",
    "            \n",
    "            res_6 = downsample_6 + block_4_6\n",
    "            \n",
    "            #downsample_7 = self.downsample(res_6, 512, 512)\n",
    "            #block_4_7 = self.discriminatorBlock(\n",
    "            #    res_6,\n",
    "            #    512,\n",
    "            #    512\n",
    "            #)\n",
    "            #\n",
    "            #res_7 = downsample_7 + block_4_7\n",
    "            \n",
    "            #print(\"res 7:\", res_7)\n",
    "            \n",
    "            conv_w_a = tf.get_variable(\n",
    "                \"conv_w_disc_end_3x3\",\n",
    "                [3, 3, 512, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            conv_out_1 = tf.nn.leaky_relu(\n",
    "                tf.nn.conv2d(res_6, conv_w_a, padding=\"SAME\"),\n",
    "                alpha=0.2\n",
    "            )\n",
    "            \n",
    "            conv_w_b = tf.get_variable(\n",
    "                \"conv_w_disc_end_4x4\",\n",
    "                [4, 4, 512, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            conv_out_2 = tf.nn.leaky_relu(\n",
    "                tf.nn.conv2d(conv_out_1, conv_w_b, padding=\"VALID\"),\n",
    "                alpha=0.2\n",
    "            )\n",
    "            \n",
    "            batch_size = tf.shape(conv_out_2)[0]\n",
    "            \n",
    "            conv_outputs_flat = tf.reshape(conv_out_2, shape=[batch_size, -1])\n",
    "            \n",
    "            W_fc = tf.get_variable(\n",
    "                \"fully_connected_W_disc_end\",\n",
    "                [512, 1],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            b_fc = tf.get_variable(\n",
    "                \"fully_connected_b_disc_end\",\n",
    "                [1],\n",
    "                initializer=tf.initializers.random_normal\n",
    "            )\n",
    "            \n",
    "            disc_out = tf.nn.leaky_relu(\n",
    "                tf.matmul(conv_outputs_flat, W_fc) + b_fc\n",
    "            )\n",
    "            \n",
    "            return disc_out\n",
    "            \n",
    "    def discriminatorBlock(self, V_in, num_input_channels, num_output_channels, downsample=True):\n",
    "        # V_in        --> [batch_size, height, width, num_input_channels]\n",
    "        # latent_w    --> [batch_size, 512]\n",
    "        #    num_input_channels  = number of input feature maps\n",
    "        #    num_output_channels = number of output feature maps\n",
    "        self.num_discriminator_blocks += 1\n",
    "        \n",
    "        conv_weight_a = tf.get_variable(\n",
    "            \"conv_w_disc_a_\" + str(self.num_discriminator_blocks),\n",
    "            [3, 3, num_input_channels, num_input_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        V_out_a = tf.nn.leaky_relu(\n",
    "            tf.nn.conv2d(V_in, conv_weight_a, padding=\"SAME\"),\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        conv_weight_b = tf.get_variable(\n",
    "            \"conv_w_disc_b_\" + str(self.num_discriminator_blocks),\n",
    "            [3, 3, num_input_channels, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        V_out_b = tf.nn.leaky_relu(\n",
    "            tf.nn.conv2d(V_out_a, conv_weight_b, padding=\"SAME\"),\n",
    "            alpha=0.2\n",
    "        )\n",
    "        \n",
    "        V_out = self.downsample(V_out_b, num_output_channels, num_output_channels)\n",
    "        \n",
    "        return V_out\n",
    "            \n",
    "    def styleBlock(self, V_in, latent_w, num_input_channels, num_output_channels, fm_dimension, upsample=False):\n",
    "        # V_in        --> [batch_size, height, width, num_input_channels]\n",
    "        # latent_w    --> [batch_size, 512]\n",
    "        #    num_input_channels  = number of input feature maps\n",
    "        #    num_output_channels = number of output feature maps\n",
    "        self.num_style_blocks += 1\n",
    "        \n",
    "        if upsample:\n",
    "            V_in = self.upsample(V_in)\n",
    "        \n",
    "        A = tf.get_variable(\n",
    "            \"A_style\" + str(self.num_style_blocks),\n",
    "            [512, num_input_channels],\n",
    "            #[512, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        conv_weight = tf.get_variable(\n",
    "            \"conv_w_style\" + str(self.num_style_blocks),\n",
    "            [3, 3, num_input_channels, num_output_channels],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        conv_bias = tf.get_variable(\n",
    "            \"conv_b_style\" + str(self.num_style_blocks),\n",
    "            [1, fm_dimension, fm_dimension, num_output_channels],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        \n",
    "        # Affine transformation of latent space vector.\n",
    "        scale = tf.matmul(latent_w, A)\n",
    "        \n",
    "        # Scale input feature map acros input channels by the affine transformation\n",
    "        # of the latent space input.\n",
    "        #print(\"##########\")\n",
    "        #print(\"scale input feature map\")\n",
    "        #print(\"scale\", scale)\n",
    "        #print(\"V_in\", V_in)\n",
    "        V_in_scaled = tf.einsum(\"bi,bhwi->bhwi\", scale, V_in)\n",
    "        \n",
    "        V_out = tf.nn.conv2d(V_in_scaled, conv_weight, padding=\"SAME\")\n",
    "        #print(\"V_out:\", V_out)\n",
    "        # This increases the number of weights by a factor of batch_size,\n",
    "        # which is weird.\n",
    "        #print(\"calculate sigma_j\")\n",
    "        #print(\"scale\", scale)\n",
    "        #print(\"conv_weight\", conv_weight)\n",
    "        #modul_conv_weight = tf.einsum(\"bc,hwjc->bhwjc\", scale, conv_weight)\n",
    "        modul_conv_weight = tf.einsum(\"bj,hwjc->bhwjc\", scale, conv_weight)\n",
    "        sigma_j = tf.sqrt(tf.reduce_sum(tf.square(modul_conv_weight), axis=[1, 2, 3]) + 1e-6)\n",
    "        \n",
    "        #print(\"calculate output\")\n",
    "        #print(\"V_in_scaled\", V_in_scaled)\n",
    "        #print(\"sigma_j\", sigma_j)\n",
    "        # Need to add biases and broadcast noise.\n",
    "        V_out_scaled = tf.nn.leaky_relu(\n",
    "            tf.einsum(\"bhwj,bj->bhwj\", V_out, sigma_j) + conv_bias,\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "        return V_out_scaled\n",
    "    \n",
    "    def upsample(self, V_in):\n",
    "        # Tested with the channel dimension.\n",
    "        fm_size = tf.shape(V_in)\n",
    "        batch_size = fm_size[0]\n",
    "        h = fm_size[1]\n",
    "        w = fm_size[2]\n",
    "        c = fm_size[3]\n",
    "        V_in_a = tf.concat([V_in, V_in,], axis=2)\n",
    "        V_in_b = tf.reshape(V_in_a, [batch_size, 2*h, w, c])\n",
    "\n",
    "        V_in_c = tf.transpose(V_in_b, perm=[0, 2, 1, 3])\n",
    "        V_in_d = tf.concat([V_in_c, V_in_c], axis=2)\n",
    "        V_out = tf.transpose(tf.reshape(V_in_d, [batch_size, 2*h, 2*w, c]), perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return V_out\n",
    "    \n",
    "    def downsample(self, V_in, input_channels, output_channels=None):\n",
    "        self.num_downsamples += 1\n",
    "        \n",
    "        if output_channels is None:\n",
    "            output_channels = 2*input_channels\n",
    "        \n",
    "        channel_increase = tf.get_variable(\n",
    "            \"channel_increaser\" + str(self.num_downsamples),\n",
    "            [1, 1, input_channels, output_channels]\n",
    "        )\n",
    "        \n",
    "        V_larger = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, channel_increase, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        V_out = tf.nn.max_pool2d(V_larger, ksize=2, strides=2, padding=\"VALID\")\n",
    "        return V_out\n",
    "    \n",
    "    def toRgb(self, V_in, h, w, c):\n",
    "        '''\n",
    "        Convert an NxNxC output block to an RGB image with dimensions\n",
    "        NxNx3.\n",
    "        '''\n",
    "        \n",
    "        self.num_to_rgbs += 1\n",
    "\n",
    "        to_rgb = tf.get_variable(\n",
    "            \"to_rgb\" + str(self.num_to_rgbs),\n",
    "            [h, w, c, 3],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        #print(\"###############\")\n",
    "        #print(\"V_in:\", V_in)\n",
    "        #print(\"to_rgb:\", to_rgb)\n",
    "        rgb_out = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, to_rgb, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        return rgb_out\n",
    "    \n",
    "    def fromRgb(self, V_in, h, w, c):\n",
    "        '''\n",
    "        Convert an NxNx3 output block to an feature map with dimensions\n",
    "        NxNxC.\n",
    "        '''\n",
    "        \n",
    "        self.num_from_rgbs += 1\n",
    "        \n",
    "        from_rgb = tf.get_variable(\n",
    "            \"from_rgb\" + str(self.num_from_rgbs),\n",
    "            [h, w, 3, c],\n",
    "            initializer=tf.initializers.random_normal\n",
    "        )\n",
    "        \n",
    "        feature_map_out = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, from_rgb, padding=\"SAME\")\n",
    "        )\n",
    "        \n",
    "        return feature_map_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiled constant input: Tensor(\"generator/Tile:0\", shape=(64, 4, 4, 512), dtype=float32)\n",
      "block_64_2: Tensor(\"generator/LeakyRelu_8:0\", shape=(64, 64, 64, 256), dtype=float32)\n",
      "Tensor(\"generator/add_31:0\", shape=(64, 256, 256, 3), dtype=float32)\n",
      "from rgb: Tensor(\"discriminator/Relu:0\", shape=(64, 256, 256, 16), dtype=float32)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "res 1: Tensor(\"discriminator/add:0\", shape=(64, 128, 128, 32), dtype=float32)\n",
      "res 2: Tensor(\"discriminator/add_1:0\", shape=(64, 64, 64, 64), dtype=float32)\n",
      "from rgb: Tensor(\"discriminator_1/Relu:0\", shape=(?, 256, 256, 16), dtype=float32)\n",
      "res 1: Tensor(\"discriminator_1/add:0\", shape=(?, 128, 128, 32), dtype=float32)\n",
      "res 2: Tensor(\"discriminator_1/add_1:0\", shape=(?, 64, 64, 64), dtype=float32)\n",
      "from rgb: Tensor(\"discriminator_2/Relu:0\", shape=(64, 256, 256, 16), dtype=float32)\n",
      "res 1: Tensor(\"discriminator_2/add:0\", shape=(64, 128, 128, 32), dtype=float32)\n",
      "res 2: Tensor(\"discriminator_2/add_1:0\", shape=(64, 64, 64, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "s = stylegan(sess)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69998 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "gan_data_generator = ImageDataGenerator(\n",
    "    rescale=1,\n",
    "    horizontal_flip=True,\n",
    ")\n",
    "\n",
    "data_flow = gan_data_generator.flow_from_directory(\n",
    "    '/home/jg/Documents/stylegan/ffhq-dataset/thisfolderisjustforkeras',\n",
    "    target_size=(256, 256),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train = True\n",
    "\n",
    "num_images = 0\n",
    "num_epochs = 0\n",
    "losses = []\n",
    "iterations = 0\n",
    "if train:\n",
    "    for x, y in data_flow:\n",
    "        print(iterations)\n",
    "        #num_images += x.shape[0]\n",
    "        #print(num_images)\n",
    "        if num_epochs == 20:\n",
    "            break\n",
    "        if x.shape[0] != 64:\n",
    "            num_epochs += 1\n",
    "            continue\n",
    "        fetches = [s.disc_loss, s.optimizer]\n",
    "        feeds = {s.true_images_ph: x}\n",
    "        \n",
    "        loss, _ = sess.run(fetches, feed_dict=feeds)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if iterations % 100 == 0:\n",
    "            plt.figure()\n",
    "            plt.plot(losses)\n",
    "            plt.xlabel(\"Num iterations\")\n",
    "            plt.ylabel(\"loss (WGAN)\")\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        \n",
    "        iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(\n",
    "    np.array(\n",
    "        [\n",
    "            [[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9]\n",
    "            ],\n",
    "            [[-1,-2,-3],\n",
    "             [-4,-5,-6],\n",
    "             [-7,-8,-9]\n",
    "            ],\n",
    "            [[1.2,2.2,3.2],\n",
    "             [4.2,5.2,6.2],\n",
    "             [7.2,8.2,9.2]\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.concat([a, a,], axis=2)\n",
    "c = tf.reshape(b, [3,6,3])\n",
    "\n",
    "d = tf.transpose(c, perm=[0, 2, 1])\n",
    "e = tf.concat([d, d], axis=2)\n",
    "f = tf.transpose(tf.reshape(e, [3, 6, 6]), perm=[0, 2, 1])\n",
    "\n",
    "g = tf.stack([a, 2*a, 3.4*a], axis=3)\n",
    "h = tf.concat([g, g], axis=2)\n",
    "i = tf.reshape(h, [3, 6, 3, 3])\n",
    "j = tf.transpose(i, perm=[0, 2, 1, 3])\n",
    "k = tf.concat([j, j], axis=2)\n",
    "l = tf.transpose(tf.reshape(k, [3, 6, 6, 3]), perm=[0, 2, 1, 3])\n",
    "\n",
    "for i in range(3):\n",
    "    print(sess.run(l)[:, :, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
