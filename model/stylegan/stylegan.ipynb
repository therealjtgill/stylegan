{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class stylegan(object):\n",
    "    def __init__(self, session, output_resolution=256):\n",
    "        self.sess = session\n",
    "        self.output_resolution = output_resolution\n",
    "        self.num_style_blocks = 0\n",
    "        self.num_to_rgbs = 0\n",
    "        \n",
    "        self.conv_weights = []\n",
    "        self.w_transforms = []\n",
    "        \n",
    "        self.latent_w = tf.placeholder(\n",
    "            [None, 512]\n",
    "        )\n",
    "        \n",
    "    def generator(self, W_in):\n",
    "        with tf.variable_scope(\"generator\") as vs:\n",
    "            self.constant_input = tf.get_variable(\n",
    "                \"c_1\",\n",
    "                [4, 4, 512],\n",
    "                initializer=tf.initializers.orthogonal\n",
    "            )\n",
    "            \n",
    "            block_4_2 = self.styleBlock(\n",
    "                self.constant_input,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512\n",
    "            )\n",
    "            \n",
    "            block_8_1 = self.styleBlock(\n",
    "                block_4_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            to_rgb_1 = self.toRgb(block_8_1)\n",
    "            \n",
    "            block_8_2 = self.styleBlock(\n",
    "                block_8_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512\n",
    "            )\n",
    "            \n",
    "            block_16_1 = self.styleBlock(\n",
    "                block_8_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_16_2 = self.styleBlock(\n",
    "                block_16_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "            )\n",
    "            \n",
    "            block_32_1 = self.styleBlock(\n",
    "                block_16_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_32_2 = self.styleBlock(\n",
    "                block_32_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "            )\n",
    "            \n",
    "            block_64_1 = self.styleBlock(\n",
    "                block_32_2,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=512,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_64_2 = self.styleBlock(\n",
    "                block_64_1,\n",
    "                W_in,\n",
    "                num_input_channels=512,\n",
    "                num_output_channels=256,\n",
    "            )\n",
    "            \n",
    "            block_128_1 = self.styleBlock(\n",
    "                block_64_2,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=256,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_128_2 = self.styleBlock(\n",
    "                block_128_1,\n",
    "                W_in,\n",
    "                num_input_channels=256,\n",
    "                num_output_channels=128,\n",
    "            )\n",
    "            \n",
    "            block_256_1 = self.styleBlock(\n",
    "                block_128_2,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=128,\n",
    "                upsample=True\n",
    "            )\n",
    "            \n",
    "            block_256_2 = self.styleBlock(\n",
    "                block_256_1,\n",
    "                W_in,\n",
    "                num_input_channels=128,\n",
    "                num_output_channels=64,\n",
    "            )\n",
    "            \n",
    "            \n",
    "            \n",
    "    def styleBlock(self, V_in, latent_w, num_input_channels, num_output_channels, upsample=False):\n",
    "        # V_in        --> [batch_size, height, width, num_input_channels]\n",
    "        # latent_w    --> [batch_size, 512]\n",
    "        # conv_weight --> [num_in_fm, num_out_fm, conv_dim]\n",
    "        #    num_in_fm  = number of input feature maps\n",
    "        #    num_out_fm = number of output feature maps\n",
    "        #    conv_dim   = dimension of convolution\n",
    "        self.num_style_blocks += 1\n",
    "        \n",
    "        if upsample:\n",
    "            V_in = self.upsample(V_in)\n",
    "        \n",
    "        A = tf.get_variable(\n",
    "            \"A_style\" + str(self.num_style_blocks),\n",
    "            [512, num_input_feature_maps],\n",
    "            initializer=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        conv_weight = tf.get_variable(\n",
    "            \"conv_w_style\" + str(self.num_style_blocks),\n",
    "            [3, 3, num_input_channels, num_output_channels],\n",
    "            initialier=tf.initializers.orthogonal\n",
    "        )\n",
    "        \n",
    "        # Affine transformation of latent space vector.\n",
    "        scale = tf.matmul(A, latent_w)\n",
    "        \n",
    "        # Scale input feature map acros input channels by the affine transformation\n",
    "        # of the latent space input.\n",
    "        V_in_scaled = tf.einsum(\"bi,bhwi->bhwi\", scale, V_in)\n",
    "        \n",
    "        V_out = tf.nn.conv2d(V_in_scaled, conv_weight, padding=\"same\")\n",
    "        \n",
    "        # This increases the number of weights by a factor of batch_size,\n",
    "        # which is weird.\n",
    "        modul_conv_weight = tf.einsum(\"bi,ijk->bijk\", scale, conv_weight)\n",
    "        sigma_j = tf.sqrt(tf.reduce_sum(tf.square(modul_conv_weight), axis=[1, 3]) + 1e-6)\n",
    "        \n",
    "        # Need to add biases and broadcast noise.\n",
    "        V_out_scaled = tf.nn.leaky_relu(\n",
    "            tf.einsum(\"bhwj,bj->bhwj\", V_in_scaled, sigma_j),\n",
    "            alpha=0.2\n",
    "        )\n",
    "\n",
    "        return V_out_scaled\n",
    "    \n",
    "    def upsample(self, V_in):\n",
    "        # Tested with the channel dimension.\n",
    "        fm_size = tf.shape(V_in)\n",
    "        batch_size, h, w, c = fm_size\n",
    "        V_in_a = tf.concat([V_in, V_in,], axis=2)\n",
    "        V_in_b = tf.reshape(V_in_a, [batch_size, 2*h, w, c])\n",
    "\n",
    "        V_in_c = tf.transpose(V_in_b, perm=[0, 2, 1, 3])\n",
    "        V_in_d = tf.concat([V_in_c, V_in_c], axis=2)\n",
    "        V_out = tf.transpose(tf.reshape(e, [batch_size, 2*h, 2*w, c]), perm=[0, 2, 1, 3])\n",
    "        \n",
    "        return V_out\n",
    "    \n",
    "    def toRgb(self, V_in):\n",
    "        '''\n",
    "        Convert an NxNxC output block to an RGB image with dimensions\n",
    "        NxNx3.\n",
    "        '''\n",
    "        \n",
    "        self.num_to_rgbs += 1\n",
    "        \n",
    "        V_in_shape = tf.shape(V_in)\n",
    "        batch_size, h, w, c = V_in_shape\n",
    "        to_rgb = tf.get_variable(\n",
    "            \"to_rgb\" + str(self.num_to_rgbs),\n",
    "            [h, w, c, 3],\n",
    "            initializer=tf.random_normal(stdev=0.2)\n",
    "        )\n",
    "        \n",
    "        rgb_out = tf.nn.relu(\n",
    "            tf.nn.conv2d(V_in, to_rgb, padding=\"same\")\n",
    "        )\n",
    "        \n",
    "        return rgb_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(\n",
    "    np.array(\n",
    "        [\n",
    "            [[1,2,3],\n",
    "             [4,5,6],\n",
    "             [7,8,9]\n",
    "            ],\n",
    "            [[-1,-2,-3],\n",
    "             [-4,-5,-6],\n",
    "             [-7,-8,-9]\n",
    "            ],\n",
    "            [[1.2,2.2,3.2],\n",
    "             [4.2,5.2,6.2],\n",
    "             [7.2,8.2,9.2]\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.   1.   2.   2.   3.   3. ]\n",
      "  [ 1.   1.   2.   2.   3.   3. ]\n",
      "  [ 4.   4.   5.   5.   6.   6. ]\n",
      "  [ 4.   4.   5.   5.   6.   6. ]\n",
      "  [ 7.   7.   8.   8.   9.   9. ]\n",
      "  [ 7.   7.   8.   8.   9.   9. ]]\n",
      "\n",
      " [[-1.  -1.  -2.  -2.  -3.  -3. ]\n",
      "  [-1.  -1.  -2.  -2.  -3.  -3. ]\n",
      "  [-4.  -4.  -5.  -5.  -6.  -6. ]\n",
      "  [-4.  -4.  -5.  -5.  -6.  -6. ]\n",
      "  [-7.  -7.  -8.  -8.  -9.  -9. ]\n",
      "  [-7.  -7.  -8.  -8.  -9.  -9. ]]\n",
      "\n",
      " [[ 1.2  1.2  2.2  2.2  3.2  3.2]\n",
      "  [ 1.2  1.2  2.2  2.2  3.2  3.2]\n",
      "  [ 4.2  4.2  5.2  5.2  6.2  6.2]\n",
      "  [ 4.2  4.2  5.2  5.2  6.2  6.2]\n",
      "  [ 7.2  7.2  8.2  8.2  9.2  9.2]\n",
      "  [ 7.2  7.2  8.2  8.2  9.2  9.2]]]\n",
      "[[[  2.    2.    4.    4.    6.    6. ]\n",
      "  [  2.    2.    4.    4.    6.    6. ]\n",
      "  [  8.    8.   10.   10.   12.   12. ]\n",
      "  [  8.    8.   10.   10.   12.   12. ]\n",
      "  [ 14.   14.   16.   16.   18.   18. ]\n",
      "  [ 14.   14.   16.   16.   18.   18. ]]\n",
      "\n",
      " [[ -2.   -2.   -4.   -4.   -6.   -6. ]\n",
      "  [ -2.   -2.   -4.   -4.   -6.   -6. ]\n",
      "  [ -8.   -8.  -10.  -10.  -12.  -12. ]\n",
      "  [ -8.   -8.  -10.  -10.  -12.  -12. ]\n",
      "  [-14.  -14.  -16.  -16.  -18.  -18. ]\n",
      "  [-14.  -14.  -16.  -16.  -18.  -18. ]]\n",
      "\n",
      " [[  2.4   2.4   4.4   4.4   6.4   6.4]\n",
      "  [  2.4   2.4   4.4   4.4   6.4   6.4]\n",
      "  [  8.4   8.4  10.4  10.4  12.4  12.4]\n",
      "  [  8.4   8.4  10.4  10.4  12.4  12.4]\n",
      "  [ 14.4  14.4  16.4  16.4  18.4  18.4]\n",
      "  [ 14.4  14.4  16.4  16.4  18.4  18.4]]]\n",
      "[[[  3.4    3.4    6.8    6.8   10.2   10.2 ]\n",
      "  [  3.4    3.4    6.8    6.8   10.2   10.2 ]\n",
      "  [ 13.6   13.6   17.    17.    20.4   20.4 ]\n",
      "  [ 13.6   13.6   17.    17.    20.4   20.4 ]\n",
      "  [ 23.8   23.8   27.2   27.2   30.6   30.6 ]\n",
      "  [ 23.8   23.8   27.2   27.2   30.6   30.6 ]]\n",
      "\n",
      " [[ -3.4   -3.4   -6.8   -6.8  -10.2  -10.2 ]\n",
      "  [ -3.4   -3.4   -6.8   -6.8  -10.2  -10.2 ]\n",
      "  [-13.6  -13.6  -17.   -17.   -20.4  -20.4 ]\n",
      "  [-13.6  -13.6  -17.   -17.   -20.4  -20.4 ]\n",
      "  [-23.8  -23.8  -27.2  -27.2  -30.6  -30.6 ]\n",
      "  [-23.8  -23.8  -27.2  -27.2  -30.6  -30.6 ]]\n",
      "\n",
      " [[  4.08   4.08   7.48   7.48  10.88  10.88]\n",
      "  [  4.08   4.08   7.48   7.48  10.88  10.88]\n",
      "  [ 14.28  14.28  17.68  17.68  21.08  21.08]\n",
      "  [ 14.28  14.28  17.68  17.68  21.08  21.08]\n",
      "  [ 24.48  24.48  27.88  27.88  31.28  31.28]\n",
      "  [ 24.48  24.48  27.88  27.88  31.28  31.28]]]\n"
     ]
    }
   ],
   "source": [
    "b = tf.concat([a, a,], axis=2)\n",
    "c = tf.reshape(b, [3,6,3])\n",
    "\n",
    "d = tf.transpose(c, perm=[0, 2, 1])\n",
    "e = tf.concat([d, d], axis=2)\n",
    "f = tf.transpose(tf.reshape(e, [3, 6, 6]), perm=[0, 2, 1])\n",
    "\n",
    "g = tf.stack([a, 2*a, 3.4*a], axis=3)\n",
    "h = tf.concat([g, g], axis=2)\n",
    "i = tf.reshape(h, [3, 6, 3, 3])\n",
    "j = tf.transpose(i, perm=[0, 2, 1, 3])\n",
    "k = tf.concat([j, j], axis=2)\n",
    "l = tf.transpose(tf.reshape(k, [3, 6, 6, 3]), perm=[0, 2, 1, 3])\n",
    "\n",
    "for i in range(3):\n",
    "    print(sess.run(l)[:, :, :, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1. ,  2. ,  3. ],\n",
       "        [ 1. ,  2. ,  3. ],\n",
       "        [ 4. ,  5. ,  6. ],\n",
       "        [ 4. ,  5. ,  6. ],\n",
       "        [ 7. ,  8. ,  9. ],\n",
       "        [ 7. ,  8. ,  9. ]],\n",
       "\n",
       "       [[-1. , -2. , -3. ],\n",
       "        [-1. , -2. , -3. ],\n",
       "        [-4. , -5. , -6. ],\n",
       "        [-4. , -5. , -6. ],\n",
       "        [-7. , -8. , -9. ],\n",
       "        [-7. , -8. , -9. ]],\n",
       "\n",
       "       [[ 1.2,  2.2,  3.2],\n",
       "        [ 1.2,  2.2,  3.2],\n",
       "        [ 4.2,  5.2,  6.2],\n",
       "        [ 4.2,  5.2,  6.2],\n",
       "        [ 7.2,  8.2,  9.2],\n",
       "        [ 7.2,  8.2,  9.2]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1. ,  1. ,  2. ,  2. ,  3. ,  3. ],\n",
       "        [ 1. ,  1. ,  2. ,  2. ,  3. ,  3. ],\n",
       "        [ 4. ,  4. ,  5. ,  5. ,  6. ,  6. ],\n",
       "        [ 4. ,  4. ,  5. ,  5. ,  6. ,  6. ],\n",
       "        [ 7. ,  7. ,  8. ,  8. ,  9. ,  9. ],\n",
       "        [ 7. ,  7. ,  8. ,  8. ,  9. ,  9. ]],\n",
       "\n",
       "       [[-1. , -1. , -2. , -2. , -3. , -3. ],\n",
       "        [-1. , -1. , -2. , -2. , -3. , -3. ],\n",
       "        [-4. , -4. , -5. , -5. , -6. , -6. ],\n",
       "        [-4. , -4. , -5. , -5. , -6. , -6. ],\n",
       "        [-7. , -7. , -8. , -8. , -9. , -9. ],\n",
       "        [-7. , -7. , -8. , -8. , -9. , -9. ]],\n",
       "\n",
       "       [[ 1.2,  1.2,  2.2,  2.2,  3.2,  3.2],\n",
       "        [ 1.2,  1.2,  2.2,  2.2,  3.2,  3.2],\n",
       "        [ 4.2,  4.2,  5.2,  5.2,  6.2,  6.2],\n",
       "        [ 4.2,  4.2,  5.2,  5.2,  6.2,  6.2],\n",
       "        [ 7.2,  7.2,  8.2,  8.2,  9.2,  9.2],\n",
       "        [ 7.2,  7.2,  8.2,  8.2,  9.2,  9.2]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function reshape in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "reshape(tensor, shape, name=None)\n",
      "    Reshapes a tensor.\n",
      "    \n",
      "    Given `tensor`, this operation returns a tensor that has the same values\n",
      "    as `tensor` with shape `shape`.\n",
      "    \n",
      "    If one component of `shape` is the special value -1, the size of that dimension\n",
      "    is computed so that the total size remains constant.  In particular, a `shape`\n",
      "    of `[-1]` flattens into 1-D.  At most one component of `shape` can be -1.\n",
      "    \n",
      "    If `shape` is 1-D or higher, then the operation returns a tensor with shape\n",
      "    `shape` filled with the values of `tensor`. In this case, the number of elements\n",
      "    implied by `shape` must be the same as the number of elements in `tensor`.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```\n",
      "    # tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "    # tensor 't' has shape [9]\n",
      "    reshape(t, [3, 3]) ==> [[1, 2, 3],\n",
      "                            [4, 5, 6],\n",
      "                            [7, 8, 9]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1], [2, 2]],\n",
      "    #                [[3, 3], [4, 4]]]\n",
      "    # tensor 't' has shape [2, 2, 2]\n",
      "    reshape(t, [2, 4]) ==> [[1, 1, 2, 2],\n",
      "                            [3, 3, 4, 4]]\n",
      "    \n",
      "    # tensor 't' is [[[1, 1, 1],\n",
      "    #                 [2, 2, 2]],\n",
      "    #                [[3, 3, 3],\n",
      "    #                 [4, 4, 4]],\n",
      "    #                [[5, 5, 5],\n",
      "    #                 [6, 6, 6]]]\n",
      "    # tensor 't' has shape [3, 2, 3]\n",
      "    # pass '[-1]' to flatten 't'\n",
      "    reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]\n",
      "    \n",
      "    # -1 can also be used to infer the shape\n",
      "    \n",
      "    # -1 is inferred to be 9:\n",
      "    reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 2:\n",
      "    reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "                             [4, 4, 4, 5, 5, 5, 6, 6, 6]]\n",
      "    # -1 is inferred to be 3:\n",
      "    reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],\n",
      "                                  [2, 2, 2],\n",
      "                                  [3, 3, 3]],\n",
      "                                 [[4, 4, 4],\n",
      "                                  [5, 5, 5],\n",
      "                                  [6, 6, 6]]]\n",
      "    \n",
      "    # tensor 't' is [7]\n",
      "    # shape `[]` reshapes to a scalar\n",
      "    reshape(t, []) ==> 7\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      tensor: A `Tensor`.\n",
      "      shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        Defines the shape of the output tensor.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function tile in module tensorflow.python.ops.gen_array_ops:\n",
      "\n",
      "tile(input, multiples, name=None)\n",
      "    Constructs a tensor by tiling a given tensor.\n",
      "    \n",
      "    This operation creates a new tensor by replicating `input` `multiples` times.\n",
      "    The output tensor's i'th dimension has `input.dims(i) * multiples[i]` elements,\n",
      "    and the values of `input` are replicated `multiples[i]` times along the 'i'th\n",
      "    dimension. For example, tiling `[a b c d]` by `[2]` produces\n",
      "    `[a b c d a b c d]`.\n",
      "    \n",
      "    Args:\n",
      "      input: A `Tensor`. 1-D or higher.\n",
      "      multiples: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "        1-D. Length must be the same as the number of dimensions in `input`\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor`. Has the same type as `input`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.tile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function transpose in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "transpose(a, perm=None, name='transpose', conjugate=False)\n",
      "    Transposes `a`.\n",
      "    \n",
      "    Permutes the dimensions according to `perm`.\n",
      "    \n",
      "    The returned tensor's dimension i will correspond to the input dimension\n",
      "    `perm[i]`. If `perm` is not given, it is set to (n-1...0), where n is\n",
      "    the rank of the input tensor. Hence by default, this operation performs a\n",
      "    regular matrix transpose on 2-D input Tensors. If conjugate is True and\n",
      "    `a.dtype` is either `complex64` or `complex128` then the values of `a`\n",
      "    are conjugated and transposed.\n",
      "    \n",
      "    @compatibility(numpy)\n",
      "    In `numpy` transposes are memory-efficient constant time operations as they\n",
      "    simply return a new view of the same data with adjusted `strides`.\n",
      "    \n",
      "    TensorFlow does not support strides, so `transpose` returns a new tensor with\n",
      "    the items permuted.\n",
      "    @end_compatibility\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    x = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
      "    tf.transpose(x)  # [[1, 4]\n",
      "                     #  [2, 5]\n",
      "                     #  [3, 6]]\n",
      "    \n",
      "    # Equivalently\n",
      "    tf.transpose(x, perm=[1, 0])  # [[1, 4]\n",
      "                                  #  [2, 5]\n",
      "                                  #  [3, 6]]\n",
      "    \n",
      "    # If x is complex, setting conjugate=True gives the conjugate transpose\n",
      "    x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],\n",
      "                     [4 + 4j, 5 + 5j, 6 + 6j]])\n",
      "    tf.transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],\n",
      "                                     #  [2 - 2j, 5 - 5j],\n",
      "                                     #  [3 - 3j, 6 - 6j]]\n",
      "    \n",
      "    # 'perm' is more useful for n-dimensional tensors, for n > 2\n",
      "    x = tf.constant([[[ 1,  2,  3],\n",
      "                      [ 4,  5,  6]],\n",
      "                     [[ 7,  8,  9],\n",
      "                      [10, 11, 12]]])\n",
      "    \n",
      "    # Take the transpose of the matrices in dimension-0\n",
      "    # (this common operation has a shorthand `linalg.matrix_transpose`)\n",
      "    tf.transpose(x, perm=[0, 2, 1])  # [[[1,  4],\n",
      "                                     #   [2,  5],\n",
      "                                     #   [3,  6]],\n",
      "                                     #  [[7, 10],\n",
      "                                     #   [8, 11],\n",
      "                                     #   [9, 12]]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      a: A `Tensor`.\n",
      "      perm: A permutation of the dimensions of `a`.\n",
      "      name: A name for the operation (optional).\n",
      "      conjugate: Optional bool. Setting it to `True` is mathematically equivalent\n",
      "        to tf.math.conj(tf.transpose(input)).\n",
      "    \n",
      "    Returns:\n",
      "      A transposed `Tensor`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.transpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function stack in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "stack(values, axis=0, name='stack')\n",
      "    Stacks a list of rank-`R` tensors into one rank-`(R+1)` tensor.\n",
      "    \n",
      "    Packs the list of tensors in `values` into a tensor with rank one higher than\n",
      "    each tensor in `values`, by packing them along the `axis` dimension.\n",
      "    Given a list of length `N` of tensors of shape `(A, B, C)`;\n",
      "    \n",
      "    if `axis == 0` then the `output` tensor will have the shape `(N, A, B, C)`.\n",
      "    if `axis == 1` then the `output` tensor will have the shape `(A, N, B, C)`.\n",
      "    Etc.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    x = tf.constant([1, 4])\n",
      "    y = tf.constant([2, 5])\n",
      "    z = tf.constant([3, 6])\n",
      "    tf.stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]] (Pack along first dim.)\n",
      "    tf.stack([x, y, z], axis=1)  # [[1, 2, 3], [4, 5, 6]]\n",
      "    ```\n",
      "    \n",
      "    This is the opposite of unstack.  The numpy equivalent is\n",
      "    \n",
      "    ```python\n",
      "    tf.stack([x, y, z]) = np.stack([x, y, z])\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      values: A list of `Tensor` objects with the same shape and type.\n",
      "      axis: An `int`. The axis to stack along. Defaults to the first dimension.\n",
      "        Negative values wrap around, so the valid range is `[-(R+1), R+1)`.\n",
      "      name: A name for this operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      output: A stacked `Tensor` with the same type as `values`.\n",
      "    \n",
      "    Raises:\n",
      "      ValueError: If `axis` is out of the range [-(R+1), R+1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use matmuls for upsampling, see whiteboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
